{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.0_Understanding natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.cli.download import download\n",
    "# download(model=\"en_core_web_md\")\n",
    "# spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "\n",
    "from utils.paths import make_dir_line\n",
    "from utils.info import sentences\n",
    "\n",
    "modality = 'c'\n",
    "project = 'Building Chatbots in Python'\n",
    "data = make_dir_line(modality, project)\n",
    "\n",
    "raw = data('raw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Intent classification with regex I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goodbye': ['bye', 'farewell'],\n",
       " 'greet': ['hello', 'hi', 'hey'],\n",
       " 'thankyou': ['thank', 'thx']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = {'goodbye': ['bye', 'farewell'],\n",
    "            'greet': ['hello', 'hi', 'hey'],\n",
    "            'thankyou': ['thank', 'thx']}\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'goodbye': re.compile('bye|farewell'), 'greet': re.compile('hello|hi|hey'), 'thankyou': re.compile('thank|thx')}\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary of patterns\n",
    "patterns = {}\n",
    "\n",
    "# Iterate over the keywords dictionary\n",
    "for intent, keys in keywords.items():\n",
    "    # Create regular expressions and compile them into pattern objects\n",
    "    patterns[intent] = re.compile('|'.join(keys))\n",
    "\n",
    "# Print the patterns\n",
    "print(patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Intent classification with regex II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {'default': 'default message',\n",
    "             'goodbye': 'goodbye for now',\n",
    "             'greet': 'Hello you! :)',\n",
    "             'thankyou': 'you are very welcome'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create templates\n",
    "bot_template = \"BOT : {0}\"\n",
    "user_template = \"USER : {0}\"\n",
    "\n",
    "def send_message(message):\n",
    "    # Print user_template including the user_message\n",
    "    print(user_template.format(message))\n",
    "    # Get the bot's response to the message\n",
    "    response = respond(message)\n",
    "    # Print the bot template including the bot's response.\n",
    "    print(bot_template.format(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the intent of a message\n",
    "def match_intent(message):\n",
    "    matched_intent = None\n",
    "    for intent, pattern in patterns.items():\n",
    "        # Check if the pattern occurs in the message \n",
    "        if pattern.search(message):\n",
    "            matched_intent = intent\n",
    "    return matched_intent\n",
    "\n",
    "# Define a respond function\n",
    "def respond(message):\n",
    "    # Call the match_intent function\n",
    "    intent = match_intent(message)\n",
    "    # Fall back to the default response\n",
    "    key = \"default\"\n",
    "    if intent in responses:\n",
    "        key = intent\n",
    "    return responses[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : hello!\n",
      "BOT : Hello you! :)\n",
      "USER : bye byeee\n",
      "BOT : goodbye for now\n",
      "USER : thanks very much!\n",
      "BOT : you are very welcome\n"
     ]
    }
   ],
   "source": [
    "# Send messages\n",
    "send_message(\"hello!\")\n",
    "send_message(\"bye byeee\")\n",
    "send_message(\"thanks very much!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 Entity extraction with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define find_name()\n",
    "def find_name(message):\n",
    "    name = None\n",
    "    # Create a pattern for checking if the keywords occur\n",
    "    name_keyword = re.compile('name|call')\n",
    "    # Create a pattern for finding capitalized words\n",
    "    name_pattern = re.compile('[A-Z]{1}[a-z]*')\n",
    "    if name_keyword.search(message):\n",
    "        # Get the matching words in the string\n",
    "        name_words = name_pattern.findall(message)\n",
    "        if len(name_words) > 0:\n",
    "            # Return the name if the keywords are present\n",
    "            name = ' '.join(name_words)\n",
    "    return name\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Find the name\n",
    "    name = find_name(message)\n",
    "    if name is None:\n",
    "        return \"Hi there!\"\n",
    "    else:\n",
    "        return \"Hello, {0}!\".format(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : my name is David Copperfield\n",
      "BOT : Hello, David Copperfield!\n",
      "USER : call me Ishmael\n",
      "BOT : Hello, Ishmael!\n",
      "USER : People call me Cassandra\n",
      "BOT : Hello, People Cassandra!\n"
     ]
    }
   ],
   "source": [
    "# Send messages\n",
    "send_message(\"my name is David Copperfield\")\n",
    "send_message(\"call me Ishmael\")\n",
    "send_message(\"People call me Cassandra\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.6 word vectors with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of sentences\n",
    "n_sentences = len(sentences)\n",
    "\n",
    "# Calculate the dimensionality of nlp\n",
    "embedding_dim = nlp.vocab.vectors_length       \n",
    "\n",
    "# Initialize the array with zeros: X\n",
    "X = np.zeros((n_sentences, embedding_dim))\n",
    "\n",
    "# Iterate over the sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # Pass each each sentence to the nlp object to create a document\n",
    "    doc = nlp(sentence)\n",
    "    # Save the document's .vector attribute to the corresponding row in X\n",
    "    X[idx, :] = doc.vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.8 Intent classification with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a support vector classifier\n",
    "# clf = SVC(C=1)\n",
    "\n",
    "# # Fit the classifier using the training data\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels of the test set\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Count the number of correct predictions\n",
    "# n_correct = 0\n",
    "# for i in range(len(y_test)):\n",
    "#     if y_pred[i] == y_test[i]:\n",
    "#         n_correct += 1\n",
    "\n",
    "# print(\"Predicted {0} correctly out of {1} test examples\".format(n_correct, len(y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El anterior ejercicios no fue posible desarrollarlo\n",
    "\n",
    "## 3.2.10 Using spaCy's entity recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': '2010', 'ORG': 'Google', 'PERSON': 'Mary'}\n",
      "{'DATE': '1999', 'ORG': 'MIT', 'PERSON': None}\n"
     ]
    }
   ],
   "source": [
    "# Define included_entities\n",
    "include_entities = ['DATE', 'ORG', 'PERSON']\n",
    "\n",
    "# Define extract_entities()\n",
    "def extract_entities(message):\n",
    "    # Create a dict to hold the entities\n",
    "    ents = dict.fromkeys(include_entities)\n",
    "    # Create a spacy document\n",
    "    doc = nlp(message)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in include_entities:\n",
    "            # Save interesting entities\n",
    "            ents[ent.label_] = ent.text\n",
    "    return ents\n",
    "\n",
    "print(extract_entities('friends called Mary who have worked at Google since 2010'))\n",
    "print(extract_entities('people who graduated from MIT in 1999'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.11 Assigning roles using spaCy's parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['black', 'red', 'blue']\n",
    "items = ['shoes', 'handback', 'jacket', 'jeans']\n",
    "\n",
    "def entity_type(word):\n",
    "    _type = None\n",
    "    if word.text in colors:\n",
    "        _type = \"color\"\n",
    "    elif word.text in items:\n",
    "        _type = \"item\"\n",
    "    return _type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: None has color : red\n",
      "item: jeans has color : blue\n"
     ]
    }
   ],
   "source": [
    "# Create the document\n",
    "doc = nlp(\"let's see that jacket in red and some blue jeans\")\n",
    "\n",
    "# Iterate over parents in parse tree until an item entity is found\n",
    "def find_parent_item(word):\n",
    "    # Iterate over the word's ancestors\n",
    "    for parent in word.ancestors:\n",
    "        # Check for an \"item\" entity\n",
    "        if entity_type(parent) == \"item\":\n",
    "            return parent.text\n",
    "    return None\n",
    "\n",
    "# For all color entities, find their parent item\n",
    "def assign_colors(doc):\n",
    "    # Iterate over the document\n",
    "    for word in doc:\n",
    "        # Check for \"color\" entities\n",
    "        if entity_type(word) == \"color\":\n",
    "            # Find the parent\n",
    "            item =  find_parent_item(word)\n",
    "            print(\"item: {0} has color : {1}\".format(item, word))\n",
    "\n",
    "# Assign the colors\n",
    "assign_colors(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.13 Rasa NLU\n",
    "\n",
    "No se pudieron instalar los paquetes en linux (no se probo en windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary modules\n",
    "# from rasa_nlu.converters import load_data\n",
    "# from rasa_nlu.config import RasaNLUConfig\n",
    "# from rasa_nlu.model import Trainer\n",
    "\n",
    "# # Create args dictionary\n",
    "# args = {\"pipeline\": \"spacy_sklearn\"}\n",
    "\n",
    "# # Create a configuration and trainer\n",
    "# config = RasaNLUConfig(cmdline_args=args)\n",
    "# trainer = Trainer(config)\n",
    "\n",
    "# # Load the training data\n",
    "# training_data = load_data(\"./training_data.json\")\n",
    "\n",
    "# # Create an interpreter by training the model\n",
    "# interpreter = trainer.train(training_data)\n",
    "\n",
    "# # Test the interpreter\n",
    "# print(interpreter.parse(\"I'm looking for a Mexican restaurant in the North of town\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
    "    {'intent': {'name': 'restaurant_search', 'confidence': 0.6627604390878398}, 'entities': [{'start': 18, 'end': 25, 'value': 'mexican', 'entity': 'cuisine', 'extractor': 'ner_crf'}, {'start': 44, 'end': 49, 'value': 'north', 'entity': 'location', 'extractor': 'ner_crf'}], 'intent_ranking': [{'name': 'restaurant_search', 'confidence': 0.6627604390878398}, {'name': 'goodbye', 'confidence': 0.14633725788681204}, {'name': 'affirm', 'confidence': 0.09756426473688806}, {'name': 'greet', 'confidence': 0.09333803828846025}], 'text': \"I'm looking for a Mexican restaurant in the North of town\"}\n",
    "\n",
    "## 3.2.14 Data-efficient entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary modules\n",
    "# from rasa_nlu.config import RasaNLUConfig\n",
    "# from rasa_nlu.model import Trainer\n",
    "\n",
    "# pipeline = [\n",
    "#     \"nlp_spacy\",\n",
    "#     \"tokenizer_spacy\",\n",
    "#     \"ner_crf\"\n",
    "# ]\n",
    "\n",
    "# # Create a config that uses this pipeline\n",
    "# config = RasaNLUConfig(cmdline_args={\"pipeline\": pipeline})\n",
    "\n",
    "# # Create a trainer that uses this config\n",
    "# trainer = Trainer(config)\n",
    "\n",
    "# # Create an interpreter by training the model\n",
    "# interpreter = trainer.train(training_data)\n",
    "\n",
    "# # Parse some messages\n",
    "# print(interpreter.parse(\"show me Chinese food in the centre of town\"))\n",
    "# print(interpreter.parse(\"I want an Indian restaurant in the west\"))\n",
    "# print(interpreter.parse(\"are there any good pizza places in the center?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [{'start': 28, 'end': 34, 'value': 'centre', 'entity': 'location', 'extractor': 'ner_crf'}], 'text': 'show me Chinese food in the centre of town'}\n",
    "\n",
    "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [{'start': 10, 'end': 16, 'value': 'indian', 'entity': 'cuisine', 'extractor': 'ner_crf'}, {'start': 35, 'end': 39, 'value': 'west', 'entity': 'location', 'extractor': 'ner_crf'}], 'text': 'I want an Indian restaurant in the west'}\n",
    "    \n",
    "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [{'start': 39, 'end': 45, 'value': 'center', 'entity': 'location', 'extractor': 'ner_crf'}], 'text': 'are there any good pizza places in the center?'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok_\n"
     ]
    }
   ],
   "source": [
    "print('Ok_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
